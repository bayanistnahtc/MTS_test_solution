# **Отчёт о реализации MVP RAG-сервиса с классификатором запросов**


## **1. Введение**
В отчете представлен анализ работы проделанной для реализации и внедрения модуля **Query Classification** для **Retrieval-Augmented Generation (RAG)** пайплайна. Решение основано на методологиях из статьи **EMNLP 2024** [https://aclanthology.org/2024.emnlp-main.981.pdf].

---

## **2. Цель**

### **2.1 Классификация запросов в RAG**
Модуль **Query Classification** определяет, **нужен ли поиск дополнительной информации** для того, чтобы корректно ответить на вопрос или LLM может Модуль **Query Classification** определяет, может ли LLM ответить на запрос сразу или требуется дополнительный поиск. Это снижает задержки и вычислительные затраты, а также улучшает качество ответов, поскольку запросы с недостаточным контекстом получают нужную информацию. В результате система работает быстрее и точнее.

 - Поиск данных полезен, если запрос требует фактов, которых нет в параметрах LLM (например, актуальные новости или специализированные знания).
 - Если же вся необходимая информация уже содержится в запросе (например, общеизвестные факты или математические задачи), дополнительный поиск не нужен.

Такой подход помогает **сократить ненужные обращения к базе знаний** и увеличить общую эффективность работы системы.

---

## **3. Реализация классификации запросов**

### **3.1 Исследование подходов**
Рассматривались несколько методов:
1. **Правила и эвристики** (например, определение фраз в запросе "самые свежие новости", "источники").
   - ✅ Простой, понятный подход.
   - ❌ Плохо масштабируется, сложно учесть все возможные вариации запросов.
   - В целом, может быть также частью системы, если нужны специфичные особенности
2. **ML-классификатор** (например, BERT-модель).
   - ✅ Хорошо обобщает данные, работает на новых запросах.
   - ❌ Требует размеченного датасета.

Поскольку важно **масштабирование и автоматизация**, был выбран **Transformer-классификатор**.

---

### **3.2 Подготовка датасета**

В рамках MVP мы не используем полный подход авторов к подготовке данных, а берем лишь ключевую идею: работаем с подмножеством данных и помечаем их с помощью LLM.

Использовали 2000 примеров из набора **Databricks Dolly-15k** (запросы и контекст, если он был предоставлен).

#### **Процесс разметки**
1. Формирование промпта для классификации.
2. Обрезка примеров (до 1024 токенов для запроса и 512 для контекста ) для оптимизации обработки.
3. Разметка LLM (HuggingFace Zephyr-7B) с двумя классами:
   - `"retrieval_required"` (поиск необходим)
   - `"no_retrieval_required"` (поиск не нужен)
4. Постобработка. Некоторые примеры отфильтровали вручную для повышения качества разметки.

#### **Распределение классов**
- **no_retrieval_required (0):** 1 540 примеров
- **retrieval_required (1):** 300 примеров

Хотя имеется дисбаланс классов, для MVP этого достаточно. В дальнейшем можно улучшить для повышения качества модели.

---

### **3.3 Fine-Tuning модели**
Пробовали несколько **BERT-подобных моделей**:
- `"distilbert-base-uncased"`
- `"microsoft/deberta-v3-small"`
- `"sentence-transformers/all-MiniLM-L6-v2"`
- `"sentence-transformers/all-MiniLM-L12-v2"`
- ✅ **"roberta-base" (лучший результат)**

#### **Техники к файнтюнингу**
1. ❌ **Обычный fine-tuning**
   - Долго обучается, требует много данных, склонен к деградации базовой модели.
2. ✅ **LoRA (PEFT - Parameter Efficient Fine-Tuning)**
   - Быстрее, обучается только небольшое число параметров.

Опуская детали проведенных экспериментов, мы представляем только ключевые особенности финальной модели:

- **Оптимизации:**
   * промптинг: "Determine if external knowledge is needed..."
   * Mixed Precision Training (FP16)
   * Early Stopping с patience=3

- **Метрики:**
   * **F1 Macro:** `0.69`
   * **Precision:** `0.43`
   * **Recall:** `0.63`

_(Метрики пока низковаты, но на нашей подготовленной тестовой выборке результаты нас устраивают для MVP. В будущем нам следует более глубоко работать с разметкой данных для достижения лучших результатов.)_

---

## **4. MVP RAG-системы**
Для демонстрации применения **Query Classification** модели мы интегрировали её в пайплайн RAG.


### **4.1 Датасет для ретривера**
- Использован набор данных **"bilgeyucel/seven-wonders"** из Википедии.
- Небольшой датасет (151 rows) по теме Семи чудес света.
- Используется в качестве **базы знаний для поиска** RAG.

---

### **4.2 Архитектура RAG**
Собрали пайплайн на **Haystack AI**:
```
A[Запрос] --> B{Классификатор}
B -->|Без поиска| C[Ответ LLM]
B -->|Нужен поиск| D[Поиск документов]
D --> E[Аугментация контекста]
E --> F[Ответ LLM]
```

**Компоненты пайплайна:**
- **Векторные эмбеддинги**: `"BAAI/bge-small-en-v1.5"`
- **Ретривер**: **В памяти (top_k=5)**
- **Генерация**: **Mistral API**
- **Хранилище**: `InMemoryDocumentStore`

---

## **5. Реализация API**
- **FastAPI + Uvicorn**
- **Docker (с Docker Compose)**

### **Эндпоинты**
- `POST /ask` → Основной обработчик запроса.
- `GET /health` → Проверка состояния сервиса.

**Примеры запросов:**
```json
POST /ask
{
    "query": "What are the Seven Wonders of the Ancient World?",
    "max_length": 512,
    "temperature": 0.1
}

✅ **Ответ:**

{
    "answer": "The Seven Wonders of the Ancient World are a list of ...,
    "retrieval_used": true,
    "documents_retrieved": [ ... ]
}
```

```json
POST /ask
{
    "query": "Calculate 2 + 2",
    "max_length": 512,
    "temperature": 0.1
}

✅ **Ответ:**

{
    "answer": "The sum of 2 + 2 is 4.",
    "retrieval_used": false,
    "documents_retrieved": []
}
```

---


## **6. Выводы**
- **Подготовили классификатор запросов с fine-tuning'ом (LoRA).**
- **Разработали и протестировали рабочий MVP RAG пайплайн с 'интеллектуальным' роутингом.**
- **Развернули сервис FastAPI с RESTful интерфейсом .**
- **Благодаря использованию классификацира запросов, средняя задержка обработки запроса значительно снизилась (улучшение времени отклика не менее 29%)**

---

## **7. Задачи на будущее**
- Определение ключевой цели для конктретной доменной области
- Расширение датасета и улучшение качество разметки для **Query Classification**
- Эксперименты с файнтютингом моделей.
- Улучшение RAG, например через **Hybrid Retrieval** (комбинированный поиск), оптимизация, ускорение каждого компонента RAG
- Реализация поддержи различных llm, включая локальные.
- Файнтюнинг llm для доменной области.
- Перенос с **InMemoryDocumentStore** в производительную векторную бд.
- Prompt enginearing
- Добавление мониторинга и логирования системы
- Реализация unit-тестов, нагрузочных
- Реализация сбора данных от пользователей для дальнейшего улучшения системы
- ...
---
